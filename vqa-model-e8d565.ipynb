{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6755629,"sourceType":"datasetVersion","datasetId":3888818}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-21T17:19:44.404388Z","iopub.execute_input":"2023-10-21T17:19:44.404705Z","iopub.status.idle":"2023-10-21T17:19:44.836271Z","shell.execute_reply.started":"2023-10-21T17:19:44.404681Z","shell.execute_reply":"2023-10-21T17:19:44.835177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow-gpu    #uncomment it firstly, if the code throws an error   \n# !pip install --upgrade pip     #uncomment it secondly, if the code throws an error\n\nfrom tensorflow.python.client import device_lib\nprint([x.name for x in device_lib.list_local_devices()])","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:19:44.838140Z","iopub.execute_input":"2023-10-21T17:19:44.838534Z","iopub.status.idle":"2023-10-21T17:19:58.086406Z","shell.execute_reply.started":"2023-10-21T17:19:44.838506Z","shell.execute_reply":"2023-10-21T17:19:58.085387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports\n\n\nimport json\nimport numpy as np\nimport copy\nfrom random import shuffle, seed\nimport sys\nimport os.path\nimport argparse\nimport glob\nimport scipy.io\nimport pdb\nimport string\nimport h5py\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport re\nimport cv2\nimport tensorflow as tf\nimport keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndropout_rate = 0.4\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom tqdm import tqdm\nfrom keras.applications.vgg16 import VGG16\nimport keras.activations\nimport keras.backend as kbe\nfrom keras.callbacks import EarlyStopping\nimport tensorflow.keras.layers as layers\nfrom keras.layers import Activation, Add, Concatenate, Conv1D, Dense, Dropout, Embedding, Softmax\nfrom keras.layers import Input, GlobalMaxPooling1D, Lambda, Multiply, RepeatVector, Reshape\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras.regularizers import l2\nimport pickle\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:19:58.087541Z","iopub.execute_input":"2023-10-21T17:19:58.088104Z","iopub.status.idle":"2023-10-21T17:20:15.973880Z","shell.execute_reply.started":"2023-10-21T17:19:58.088075Z","shell.execute_reply":"2023-10-21T17:20:15.973016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://figshare.com/ndownloader/files/10798046 -O GoogleNews-vectors-negative300.bin  #Use it to download on kaggle storage\n\n\nmodel_path = '/kaggle/working/GoogleNews-vectors-negative300.bin'            # Path where the model is stored\nmodel_w2v = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)        #word_to_vector model initiated","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:20:15.976419Z","iopub.execute_input":"2023-10-21T17:20:15.976854Z","iopub.status.idle":"2023-10-21T17:23:08.270595Z","shell.execute_reply.started":"2023-10-21T17:20:15.976797Z","shell.execute_reply":"2023-10-21T17:23:08.269563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/QA                 #Add it to stor .h5 files\n!mkdir /kaggle/working/VQA_Model          #Add it to store the final model","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:33:28.477550Z","iopub.execute_input":"2023-10-21T17:33:28.477995Z","iopub.status.idle":"2023-10-21T17:33:30.682539Z","shell.execute_reply.started":"2023-10-21T17:33:28.477960Z","shell.execute_reply":"2023-10-21T17:33:30.681387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_feat(doc):\n  '''\n    Input: A sentence\n    Output : Word Embedding of the sentence\n\n    Note : In order to maintain uniformity in the word embedding of the question, I have used padding (which would be shown later) with a maximum length of 21 (since this\n    is the longest length of the question in VQA_RAD Dataset Public.json)\n  '''\n  feat = []\n  for word in doc:\n      try:\n          feat.append(model_w2v[word])\n      except:\n          pass\n  return feat\n\ndef tokenize(sentence):\n\n    ''' Perform Tokenization '''\n    return [i for i in re.split(r\"([-.\\\"',:? !$#@~()*&\\^%;/\\\\+<>\\n=])\", sentence) if i!='' and i!=' ' and i!='\\n'];\n\ndef prepro_question(imgs, method):\n    # preprocess all the question\n    print('example processed tokens:')\n\n    '''\n    Input: The question from VQA_RAD Dataset Public.json\n    Performs tokenization and lowering of the question\n    Output: Embedded version of the question\n\n    Note that: We have still not padded the questions, just for information each of the word will be a 300 Dimensional Vector,\n    hence, the word vector (which will be obtained after padding) will be a (21,300) Dimensional\n    '''\n    for i,img in enumerate(imgs):\n        s = img['question'].lower()\n        if method == 'nltk':\n            txt = word_tokenize(str(s).lower())\n        else:\n            txt = tokenize(s)\n        img['processed_tokens'] = txt\n        if i < 10: print(txt)\n        if i % 1000 == 0:\n            sys.stdout.write(\"processing %d/%d (%.2f%% done)   \\r\" %  (i, len(img), i*100.0/len(imgs)) )\n            sys.stdout.flush()\n    return imgs\n\ndef get_top_answers(imgs, num_ans):\n\n    \"\"\"\n    Print the questions and returns the number of time one answer is repeated\n    \"\"\"\n    counts = {}\n    for img in imgs:\n        try:\n            ans = img['answer'].lower()  # If the string is a number, it would result into error\n        except :\n            ans = str(img['answer'])\n        counts[ans] = counts.get(ans, 0) + 1\n\n    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n    print('top answer and their counts:')\n    print('\\n'.join(map(str,cw[:20])))\n\n    vocab = []\n    for i in range(min(num_ans,len(cw))):\n        vocab.append(cw[i][1])\n\n    return vocab[:num_ans]\n\n\ndef filter_question(imgs, atoi):\n\n  ''' Not of much use, I had used it for some other purpose, but did not use it later '''\n  new_imgs = []\n  for i, img in enumerate(imgs):\n          new_imgs.append(img)\n\n  print('question number reduce from %d to %d '%(len(imgs), len(new_imgs)))\n  return new_imgs\n\nmanualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n                  '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n                  'eight': '8', 'nine': '9', 'ten': '10' }\n\nimgs = json.load(open('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Dataset Public.json' , 'r'))      \nnum_ans = 1000\ntop_ans = get_top_answers(imgs, num_ans)\natoi = {w:i for i,w in enumerate(top_ans)}                   # Word : Count\nitoa = {i:w for i,w in enumerate(top_ans)}                   # Count : Word\nfeat_dim = 300                                               # 300 Dimensional Vector\nimgs_data = json.load(open('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Dataset Public.json' , 'r'))  # VQA_RAD Dataset Public.json\nnum_ans = 10    # Even 1 should work fine, but I had taken reference from COCO dataset, and hence, 10 (10 represents the top 10 answers to a picture)\nmethod = 'nltk'\nmax_length = 21                    # Max Length of the question\ndir_path = \"/kaggle/working/QA\"    # The path where we will be storing .h5 file\nN = len(imgs_data)\n\nimage_path = '/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Image Folder'\n\ndef save_data():\n\n\n        for i,img in enumerate(imgs_data):\n           #print('X' , img['ques_id'])\n            img_path = image_path+img['image_name']\n\n            s = img['question']\n            print(i,s)   # Print the number and the question\n            if method == 'nltk':\n                try:\n                    txt = word_tokenize(str(s).lower())\n                except :\n                    txt = str(s)\n            else:\n                    txt = tokenize(s)\n\n            img['processed_tokens'] = txt\n            question_id = img['qid']\n            feat = np.array(extract_feat(img['processed_tokens']))\n            label_arrays = np.zeros((1, max_length, feat_dim), dtype='float32')\n            label_length = min(max_length, len(feat)) # record the length of this sequence\n            label_arrays[0, :label_length, :] = feat\n            try:\n                ans_arrays = atoi[img['answer'].lower()]\n            except :\n                ans_arrays = atoi[str(img['answer'])]\n\n            f = h5py.File(os.path.join( dir_path , str(question_id) + '.h5'), \"w\")\n            f.create_dataset(\"ques\", dtype='float32', data=label_arrays)\n            f.create_dataset(\"answers\", dtype='uint32', data=ans_arrays)\n            f.close()\n        return\n\ndata = save_data()","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:33:30.685090Z","iopub.execute_input":"2023-10-21T17:33:30.685397Z","iopub.status.idle":"2023-10-21T17:33:33.052352Z","shell.execute_reply.started":"2023-10-21T17:33:30.685370Z","shell.execute_reply":"2023-10-21T17:33:33.051340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_layer(input_shape):\n    '''\n    Input : Shape of the image\n    Output : VGG16 Preprocessing model\n    '''\n    base_model = tf.keras.applications.VGG16(input_shape=input_shape, include_top=False,weights='imagenet')\n    base_model.trainable = False      # Do not train it\n    x = base_model.layers[-2].output  # Shape would be (28*28*512)\n    x = tf.reshape(x , [-1,x.shape[2]*x.shape[1] , x.shape[3]]) # Shape would be (1,784,512)\n    x = tf.keras.layers.Dense(1024)\n    return x\n\ndef vgg_preprocessing(model,image):\n  ''' Takes a tensor as an input, and returns a pre processed version of the image'''\n  return model(image)\n\ndef load_data():\n\n        '''\n        Input: Nothing\n\n        Output: Returns list containing the following four elements in a tuple\n        (preprocessed version of the image,embedded question,embedded answer, question id)\n        '''\n\n        images = []\n        questions = []\n        answers = []\n        ids = []\n\n        data = imgs_data   # VQA_RAD Dataset Public.json\n        model = image_layer(input_shape = (448,448,3)) # Making VGG16 Model\n        for i,img in enumerate(data):\n\n            img_path = img['image_name']  # Image Name\n            question_id = img['qid']      # Question id\n\n            #label_arrays = np.zeros((1, max_length, feat_dim), dtype='float32') # Somethings are taken directly from\n\n            with h5py.File(os.path.join(dir_path,str(question_id) + '.h5'),'r') as hf:\n                question = hf['ques'][:]           # Embedded question\n                answer = hf['answers'][()]               # Embedded answer\n\n\n            image = cv2.imread(os.path.join('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Image Folder',img_path) , cv2.IMREAD_COLOR) # Reading the image\n            image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image , (448,448)) # Reshape\n\n            '''\n            Small Note : I had not applied the VGG Preprocessing here, because I wanted to use the original shape of 448, and then, I would\n            attach the VGG Model, while making the model, I think this makes sense :)\n            '''\n            #image = vgg_preprocessing(model,image)\n            # Apply VGG16 Preprocessings\n\n            images.append(image)\n            questions.append(np.array(question))\n            answers.append(np.array(answer))\n            ids.append(question_id)\n            if i%100==0:\n              print(\"Processed =>\",i,' which is',round(100*i/len(data),2),'%')\n\n        questions = np.reshape(np.array(questions) , [-1,max_length,feat_dim])\n        return (np.array(images) , questions ,np.array(answers) , np.array(ids))\n\nimgs = json.load(open('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Dataset Public.json' , 'r'))\nnum_ans = 1000\ntop_ans = get_top_answers(imgs, num_ans )\natoi = {w:i for i,w in enumerate(top_ans)}\nitoa = {i:w for i,w in enumerate(top_ans)}\nfeat_dim = 300\nimgs_data = json.load(open('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Dataset Public.json' , 'r'))\nnum_ans = 10\nmethod = 'nltk'\nmax_length = 21\ndir_path = \"/kaggle/working/QA\"\nN = len(imgs_data)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:33:33.053934Z","iopub.execute_input":"2023-10-21T17:33:33.054327Z","iopub.status.idle":"2023-10-21T17:33:33.106642Z","shell.execute_reply.started":"2023-10-21T17:33:33.054290Z","shell.execute_reply":"2023-10-21T17:33:33.105677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' These parameters are for some of the previous attempts, so ignore it, the main part is : datagen = load_data(), and I don't want to remove all of these,\nbecause indeed there can be some ideas coming out from these lines of code'''\n\nembed_size = 300\nq_len = 21\nheight = 224\nwidth = 224\nlstm_units = 256\nattention_dim = 512\nnum_output = 1000\nmax_questions = 3064\n\nbatch_size = 32\nlr = 0.001\narticles = ['a', 'an', 'the']\nmanualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n                  '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n                  'eight': '8', 'nine': '9', 'ten': '10' }\n\n\ndatagen = load_data()   # Load the Data","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:33:33.108627Z","iopub.execute_input":"2023-10-21T17:33:33.108972Z","iopub.status.idle":"2023-10-21T17:33:58.479387Z","shell.execute_reply.started":"2023-10-21T17:33:33.108936Z","shell.execute_reply":"2023-10-21T17:33:58.478440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images,questions,answers,ids =datagen[0],datagen[1],datagen[2],datagen[3]\nprint(\"Images have a size of:\",images.shape)\nprint(\"Questions have a size of:\",questions.shape)\nprint(\"Answers have a size of:\",answers.shape)\nprint(\"Ids have a size of:\",ids.shape)\ndir_path = r'/kaggle/working/QA' # The directory where the .h5 file for each entry is saved\nm = 100\nfor i in range(images.shape[0]):\n    ans_array = answers[i]\n    image_array = images[i]\n    quest_array = questions[i]\n    question_id = ids[i]\n    f = h5py.File(os.path.join( dir_path , str(question_id) + '.h5'), \"w\") # Loading the 'h5 file\n    f.create_dataset(\"ques\", dtype='float32', data=quest_array) # Question Embedding\n    f.create_dataset(\"image_vector\", dtype='float32', data=image_array) # Image Embedding (Not preprocessed)\n    f.create_dataset(\"answers\", dtype='uint32', data=ans_array)      # Answers in embedded form\n    f.close()\n    if i%m ==0:\n        print(\"Processed =>\", i,' total percentage =>', round(100*i/images.shape[0],2),' %')\nprint(\"Your processing has been done\")","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:33:58.480570Z","iopub.execute_input":"2023-10-21T17:33:58.480868Z","iopub.status.idle":"2023-10-21T17:34:11.927366Z","shell.execute_reply.started":"2023-10-21T17:33:58.480842Z","shell.execute_reply":"2023-10-21T17:34:11.925893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir = r'/kaggle/working/QA/' # Containing .h5 file\nimages = []\nans = []\nques = []\ncount = 0\ncontent = os.listdir(dir)\nlength = len(content)\nfor i in content:\n    # Reading the data\n    file = h5py.File(dir+i)\n    images.append(np.array(file['.']['image_vector'][:]))\n    ans.append(np.array(file['.']['answers'][()]))\n    ques.append(np.array(file['.']['ques'][:]))\n    count+=1\n    if count%100 == 0:\n      print(\"The count is:\",count,\"and the percentage proportion is:\",round(100*count/length,2),'%')\nimages = tf.convert_to_tensor(np.array(images))   # For the GPU purpose\nans = tf.convert_to_tensor(np.array(ans))\nques = tf.convert_to_tensor(np.array(ques))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:34:11.928699Z","iopub.execute_input":"2023-10-21T17:34:11.929141Z","iopub.status.idle":"2023-10-21T17:34:30.791146Z","shell.execute_reply.started":"2023-10-21T17:34:11.929101Z","shell.execute_reply":"2023-10-21T17:34:30.790140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''  Preprocessing with the VGG 16 Model  '''\n\nmodel = tf.keras.applications.VGG16(include_top=False,weights='imagenet',\n        input_shape=(448,448,3))\n#print(\"The Last layer\")\nlast_layer = model.layers[-1].output   # Last layer has an output layer of (14,14,512)\nmodel = Model(model.input,last_layer)\nmodel.trainable = False\n\n\ndimen_red = tf.keras.Sequential()  # Use for converting (196,512) -> (21,300)\ndimen_red.add(tf.keras.layers.Conv2D(300,kernel_size=(1,1),input_shape= (14,14,512)))\ndimen_red.add(tf.keras.layers.Reshape((196,300)))\ndimen_red.add(tf.keras.layers.Permute((2,1)))  # Reshaping about the axis, useful for applying the dense network\ndimen_red.add(tf.keras.layers.Dense(21))\ndimen_red.add(tf.keras.layers.Permute((2,1)))  # Reshaping about the axis, useful for applying the dense network","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:34:30.792492Z","iopub.execute_input":"2023-10-21T17:34:30.792868Z","iopub.status.idle":"2023-10-21T17:34:32.272856Z","shell.execute_reply.started":"2023-10-21T17:34:30.792816Z","shell.execute_reply":"2023-10-21T17:34:32.271870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nlength = images.shape[0]\nfor i,j in enumerate(images):\n  l.append(model(tf.reshape(j,[1,448,448,3])))  # It was not possible directly on GPU, hence had to use for loop\n  if i%100 ==0:\n    print(\"The count is:\",i,\"and the percentage proportion is:\",round(100*i/length,2),'%')\nimages = tf.convert_to_tensor(np.array(l) )","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:34:32.274889Z","iopub.execute_input":"2023-10-21T17:34:32.275154Z","iopub.status.idle":"2023-10-21T17:35:42.703187Z","shell.execute_reply.started":"2023-10-21T17:34:32.275131Z","shell.execute_reply":"2023-10-21T17:35:42.702076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nlength = images.shape[0]\nfor i,j in enumerate(images):\n  l.append(dimen_red(j))     # Making it to the same shape as that of question embedding\n  if i%100 ==0:\n    print(\"The count is:\",i,\"and the percentage proportion is:\",round(100*i/length,2),'%')\nimages = tf.convert_to_tensor(np.array(l))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:05.189052Z","iopub.execute_input":"2023-10-21T17:38:05.189978Z","iopub.status.idle":"2023-10-21T17:38:14.715172Z","shell.execute_reply.started":"2023-10-21T17:38:05.189943Z","shell.execute_reply":"2023-10-21T17:38:14.714029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = tf.reshape(images,[length,21,300])\nimg = images  #Tensor containing images\nque = ques  # Tensor containing question vector\nimg = img/255.0 # Normalizing","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:14.717166Z","iopub.execute_input":"2023-10-21T17:38:14.717491Z","iopub.status.idle":"2023-10-21T17:38:14.728185Z","shell.execute_reply.started":"2023-10-21T17:38:14.717462Z","shell.execute_reply":"2023-10-21T17:38:14.727264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"que.shape,img.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:22.445166Z","iopub.execute_input":"2023-10-21T17:38:22.445562Z","iopub.status.idle":"2023-10-21T17:38:22.453472Z","shell.execute_reply.started":"2023-10-21T17:38:22.445529Z","shell.execute_reply":"2023-10-21T17:38:22.452382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_ratio = 0.8                                   # Defining the split ratio (e.g., 80% training, 20% testing)\n\nsplit_index_que = int(que.shape[0] * split_ratio)    # Splitting the que tensor into training and testing\nsplit_index_img = int(img.shape[0] * split_ratio)    # Splitting the img tensor into training and testing\nque_train, que_test = tf.split(que, [split_index_que, que.shape[0] - split_index_que], axis=0)\nimg_train, img_test = tf.split(img, [split_index_img, img.shape[0] - split_index_img], axis=0)\n\nprint(\"Training question and image shape:\", que_train.shape,img_train.shape)\nprint(\"Testing question and image shape:\", que_test.shape,img_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:24.564524Z","iopub.execute_input":"2023-10-21T17:38:24.565455Z","iopub.status.idle":"2023-10-21T17:38:24.580794Z","shell.execute_reply.started":"2023-10-21T17:38:24.565412Z","shell.execute_reply":"2023-10-21T17:38:24.579878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' The below mentioned two Input objects of keras will be useful for making the model '''\n\nques = tf.keras.layers.Input((21,300))   # Input Model (for ques)\nimages = tf.keras.layers.Input((21,300)) # Input Model (for images)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:28.861529Z","iopub.execute_input":"2023-10-21T17:38:28.861905Z","iopub.status.idle":"2023-10-21T17:38:28.869987Z","shell.execute_reply.started":"2023-10-21T17:38:28.861871Z","shell.execute_reply":"2023-10-21T17:38:28.868920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D,Flatten,Concatenate\n\n''' Imagica is for the preprocessing of the image part'''\nimagica = Dense(512,activation='tanh')(images)\nimagica = Dense(512,activation='tanh')(images)\n\n\n''' quesa is for the ques layer, which means preprocessing of the question layer'''\nquesa = LSTM(512, dropout = 0.3,return_sequences = True,input_shape = (21,300))(ques)\nquesa = Dense(512, activation = 'relu')(quesa)\nquesa = Dropout(0.3)(quesa)\nquesa = Dense(512, activation = 'relu')(quesa)\nquesa = Dropout(0.3)(quesa)\n\n''' Concatenating both image and the question layer'''\nquesa = Concatenate()([quesa,imagica])\nquesa = Flatten()(quesa)\nout = tf.keras.layers.Dense(517,activation='softmax')(quesa) # Final output has 517 different categories","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:31.301329Z","iopub.execute_input":"2023-10-21T17:38:31.301965Z","iopub.status.idle":"2023-10-21T17:38:31.925325Z","shell.execute_reply.started":"2023-10-21T17:38:31.301928Z","shell.execute_reply":"2023-10-21T17:38:31.923047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nmodel = Model([ques,images],[out])\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:35.787257Z","iopub.execute_input":"2023-10-21T17:38:35.788089Z","iopub.status.idle":"2023-10-21T17:38:36.164525Z","shell.execute_reply.started":"2023-10-21T17:38:35.788053Z","shell.execute_reply":"2023-10-21T17:38:36.163447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01),loss ='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:39.825744Z","iopub.execute_input":"2023-10-21T17:38:39.826136Z","iopub.status.idle":"2023-10-21T17:38:39.844918Z","shell.execute_reply.started":"2023-10-21T17:38:39.826107Z","shell.execute_reply":"2023-10-21T17:38:39.843870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers = tf.keras.utils.to_categorical(ans)\nanswers.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:40.328017Z","iopub.execute_input":"2023-10-21T17:38:40.328835Z","iopub.status.idle":"2023-10-21T17:38:40.336156Z","shell.execute_reply.started":"2023-10-21T17:38:40.328792Z","shell.execute_reply":"2023-10-21T17:38:40.335157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer = tf.keras.utils.to_categorical(ans)\n\nsplit_ratio = 0.8                                      # Defining the split ratio (e.g., 80% training, 20% testing)\n\nsplit_index_ans = int(answer.shape[0] * split_ratio)   # Splitting the ans tensor into training and testing\nanswer_train, answer_test = tf.split(answer, [split_index_ans, answer.shape[0] - split_index_ans], axis=0)\n\nprint(\"Answers training has shape: \", answer_train.shape)\nprint(\"Answers test has shape: \", answer_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:42.880137Z","iopub.execute_input":"2023-10-21T17:38:42.880842Z","iopub.status.idle":"2023-10-21T17:38:42.890063Z","shell.execute_reply.started":"2023-10-21T17:38:42.880800Z","shell.execute_reply":"2023-10-21T17:38:42.889017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:44.966176Z","iopub.execute_input":"2023-10-21T17:38:44.966559Z","iopub.status.idle":"2023-10-21T17:38:44.998566Z","shell.execute_reply.started":"2023-10-21T17:38:44.966527Z","shell.execute_reply":"2023-10-21T17:38:44.997683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_accuracy_callback = keras.callbacks.ModelCheckpoint(\n    filepath='/kaggle/working/VQA_Model/best_model.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    save_weights_only=True,\n    mode='max',\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:47.384343Z","iopub.execute_input":"2023-10-21T17:38:47.384977Z","iopub.status.idle":"2023-10-21T17:38:47.389936Z","shell.execute_reply.started":"2023-10-21T17:38:47.384943Z","shell.execute_reply":"2023-10-21T17:38:47.388884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([img_train,que_train],answer_train,epochs = 150,batch_size=32,verbose=1,callbacks=[best_accuracy_callback])","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:38:48.892725Z","iopub.execute_input":"2023-10-21T17:38:48.893720Z","iopub.status.idle":"2023-10-21T17:40:22.020371Z","shell.execute_reply.started":"2023-10-21T17:38:48.893682Z","shell.execute_reply":"2023-10-21T17:40:22.019515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/VQA_Model')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:40:27.496941Z","iopub.execute_input":"2023-10-21T17:40:27.497319Z","iopub.status.idle":"2023-10-21T17:40:32.913877Z","shell.execute_reply.started":"2023-10-21T17:40:27.497286Z","shell.execute_reply":"2023-10-21T17:40:32.912845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = tf.argmax(model.predict([img,que]),axis=1).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:40:44.752695Z","iopub.execute_input":"2023-10-21T17:40:44.753133Z","iopub.status.idle":"2023-10-21T17:40:45.742643Z","shell.execute_reply.started":"2023-10-21T17:40:44.753099Z","shell.execute_reply":"2023-10-21T17:40:45.741798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = open('/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Dataset Public.json','r')\ntrain = json.load(x)\ntrain[0]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:41:08.752952Z","iopub.execute_input":"2023-10-21T17:41:08.753344Z","iopub.status.idle":"2023-10-21T17:41:08.799733Z","shell.execute_reply.started":"2023-10-21T17:41:08.753312Z","shell.execute_reply":"2023-10-21T17:41:08.798865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate([img_test,que_test],answer_test)    #because of very less data we got a little less accuracy","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:41:17.254319Z","iopub.execute_input":"2023-10-21T17:41:17.254714Z","iopub.status.idle":"2023-10-21T17:41:17.984844Z","shell.execute_reply.started":"2023-10-21T17:41:17.254683Z","shell.execute_reply":"2023-10-21T17:41:17.983791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}